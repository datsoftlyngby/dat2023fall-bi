{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c52bf402",
   "metadata": {},
   "source": [
    "# Read HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08fbe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another HTML parser\n",
    "# !pip3 install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167b1e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9c5de6",
   "metadata": {},
   "source": [
    "## Parcing HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d1476f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile readhtml.py\n",
    "\n",
    "import requests\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "global data\n",
    "data = dict()\n",
    "\n",
    "def readHTML(url):\n",
    "        links = []\n",
    "        text = \"\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(\"Error fetching page\")\n",
    "            exit(0)\n",
    "        else: \n",
    "            # BeautifulSoup object represents the document as a nested data structure\n",
    "            soup = BeautifulSoup(response.text, features=\"html.parser\")           \n",
    "            # soup = BeautifulSoup(response.text, features=\"lxml\")\n",
    "        \n",
    "        # get title\n",
    "        title = soup.title.get_text() if soup.title else None\n",
    "\n",
    "        # get description\n",
    "        meta = soup.find('meta', attrs={'name': 'description'})\n",
    "        description = meta.get('content') if meta else None     \n",
    "        \n",
    "        # remove the scripts and the style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract() \n",
    "\n",
    "        # parse in paragraphs\n",
    "        selectors = 'h1, h2, h3, p'\n",
    "        for para in soup.select(selectors):\n",
    "            text = \" \".join((text, str(para.get_text()))) \n",
    "        \n",
    "        # break into lines, remove leading and trailing space on each line\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "\n",
    "        # break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "\n",
    "        # drop blank lines\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)   \n",
    "        \n",
    "        links = readLinks(url)                     # internal call\n",
    "        \n",
    "        # Store the data\n",
    "        data[url] = {\n",
    "                        \"title\": title, \n",
    "                        \"description\": description,\n",
    "                        \"text\": text, \n",
    "                        \"links\": links[url]        \n",
    "                }\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    \n",
    "def readLinks(url):\n",
    "        external_links = set()\n",
    "        internal_links = set()\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        # soup = BeautifulSoup(page.content, features=\"lxml\")\n",
    "        \n",
    "        # search for specific link\n",
    "        # for link in soup.findAll('a', attrs={'href': re.compile(\"^/page/\")}):\n",
    "        \n",
    "        for line in soup.findAll('a'):\n",
    "            link = line.get('href')           \n",
    "            if not link:\n",
    "                continue\n",
    "            if link.startswith(\"javascript:\"):\n",
    "                continue\n",
    "                \n",
    "            # Remove anchors\n",
    "            link = link.split(\"#\")[0]\n",
    "            \n",
    "            # Remove parameters\n",
    "            link = link.split(\"?\")[0]\n",
    "            \n",
    "            # Remove trailing forward slash\n",
    "            link = link.rstrip(\"/\")\n",
    "            \n",
    "            if link.startswith('http'):\n",
    "                external_links.add(link)\n",
    "            else:\n",
    "                internal_links.add(link)\n",
    "\n",
    "        #for link in soup.findAll('a'):\n",
    "        #    links.append(link.get('href'))\n",
    "        #return title, links   \n",
    "\n",
    "        # Full internal links\n",
    "        full_internal_links = {urljoin(url, internal_link) for internal_link in internal_links}\n",
    "        \n",
    "        # All unique external and full internal links\n",
    "        links_list = list(external_links.union(full_internal_links))        \n",
    "            \n",
    "        return links_list   \n",
    "     \n",
    "    \n",
    "# stack is the links_list from above\n",
    "def readStack(stack):\n",
    "        print(len(stack))\n",
    "        visited = []\n",
    "\n",
    "        while stack:\n",
    "            # Visit the URL\n",
    "            current = stack.pop()\n",
    "            try:\n",
    "                url = current\n",
    "                readHTML(url)                     # call\n",
    "                visited.append(current)\n",
    "                \n",
    "            except:\n",
    "                print(f\"Couldn't open {current}\")\n",
    "                continue\n",
    "\n",
    "        return data\n",
    "                \n",
    "                \n",
    "# Read tables\n",
    "def readHTMLtab(url):\n",
    "\n",
    "            response = requests.get(url).text\n",
    "            # soup = BeautifulSoup(response, 'lxml')\n",
    "            soup = BeautifulSoup(response, 'html.parser')\n",
    "\n",
    "            title = soup.title.get_text()\n",
    "            text = soup.get_text()\n",
    "            # print(title)\n",
    "            # print(soup.prettify())  \n",
    "\n",
    "            # soup.find('table', class_='engineTable') \n",
    "            soup.find('table')\n",
    "            table_body = soup.find('tbody').text\n",
    "\n",
    "            # remove the starting \\n\n",
    "            table_body = table_body[2:-2]        \n",
    "            table_list = table_body.split('\\n\\n\\n')\n",
    "\n",
    "            for index, element in enumerate(table_list):\n",
    "                table_list[index] = element.split('\\n')\n",
    "                print(table_list)\n",
    "            return title, table_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f28156",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3807ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import readhtml\n",
    "from readhtml import readHTML, readLinks, readStack, readHTMLtab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577a5ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib \n",
    "importlib.reload(readhtml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86eaafee",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://innotechspace.dk/holodeck/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f06d138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://cphbusiness.dk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f388105",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = readHTML(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e4a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02a6e44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "readStack(data[url]['links'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae655f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['https://innotechspace.dk/holodeck/'].get('links')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c6825b",
   "metadata": {},
   "source": [
    "### Test for Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27c0e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = readLinks(url)\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53d5e87",
   "metadata": {},
   "source": [
    "### Test for Tabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed9f818",
   "metadata": {},
   "outputs": [],
   "source": [
    "title, table_list = readHTMLtab(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b0de85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data=table_list)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43266434",
   "metadata": {},
   "source": [
    "Finally, move the whole my dir to '/Users/tdi/opt/anaconda3/lib/python3.9/site-packages'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
