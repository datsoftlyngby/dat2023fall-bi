{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40458ccb",
   "metadata": {},
   "source": [
    "# Exercise: Text Vectorisation and Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7ad7b0",
   "metadata": {},
   "source": [
    "The process of converting or transforming a data set into a set of vectors is called __Vectorization__.<br>\n",
    "Here we explore several methods of text vectorisation.\n",
    "- word vectors - a relatively old approach of NLP, where the words of a sentence are processed without context, just lexically (NLP). These algorithms implement techniques, such as Bag-Of-Words and TF-IDF <br>\n",
    "- modern algoriths that consider the meaning, the semantical role of the word in a sentence and simulate \"undrstanding\" the inforation in the document (NLU). These algiriths implement tokens, embedding, and deep learning ANN.\n",
    "\n",
    "This notebook explains the __first approach__ and several methods for its implementation.\n",
    "\n",
    "Objectives: \n",
    "- understanding the basic concepts of text vectorisation \n",
    "- practicing implementation of vectorisation algorithms in Python programming\n",
    "\n",
    "Tasks: \n",
    "1. Create a function for estimating the similarity between two vectors by means of _cosine similarity_ measure\n",
    "2. Test the function by comparing variety of numeric test data\n",
    "3. Test the function by comparing text data\n",
    "3. Apply the function in a Q&A (questions answering) application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a65ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ctypes\n",
    "import math\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7da494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cosimfunc\n",
    "from cosimfunc import cosim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7ea23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib \n",
    "importlib.reload(cosimfunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ea901f",
   "metadata": {},
   "source": [
    "## Input "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4b9d31",
   "metadata": {},
   "source": [
    "First, we experiment with two text documents (sentences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af260bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter two sentences\n",
    "doc1 = \"Mette Frederiksen is the prime minister of Denmark.\"\n",
    "doc2 = \"Denmark has female prime minister, who has the name Mette Frederiksen.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd38b5fc",
   "metadata": {},
   "source": [
    "## Bag Of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b81c1ab",
   "metadata": {},
   "source": [
    "A model, which represents the text as an unordered collection of words. It doesn't considedr the grammar features, nor the order of appearance of the words, but keeps information about multiplicity of the words. <br>\n",
    "Once we collect the words, we can code them in a numeric code, so to be able to apply the similarity function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the sentences in words\n",
    "sent1 = doc1.split(\" \")\n",
    "sent2 = doc2.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbcb355",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sent1)\n",
    "print(sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25df01f5",
   "metadata": {},
   "source": [
    "### Corpus of Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef71bd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All words appearing in all documents\n",
    "# union() removes duplications\n",
    "corpus = set(sent1).union(set(sent2))\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c8432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus size\n",
    "n = len(corpus)\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a70f35",
   "metadata": {},
   "source": [
    "### Method 1: Binary Vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1ab458",
   "metadata": {},
   "source": [
    "Check if a word appears in a semntence (document).\n",
    "- For each word write 1 if it appears or 0 if not.\n",
    "- Do this for each sentence separately.\n",
    "\n",
    "Store the findings in a dictionary (key - value storucture), one dict for each sentence.\n",
    "- fron the start each dictionary will contain empty positions - one per word in the corpus\n",
    "- the word is a key, the appearance of it is a value - 1 (appear) or 0 (doesn't appear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea32d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary vector of word appearance in a sentence\n",
    "def vect(sent):\n",
    "    # create new dict and place zeros in it\n",
    "    mydict = dict.fromkeys(corpus, 0) \n",
    "    \n",
    "    # code each word's appearance in the sentence with 1\n",
    "    for word in sent:\n",
    "        mydict[word] = 1\n",
    "    return mydict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e529342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarise sentence 1\n",
    "dict1 = vect(sent1)\n",
    "dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ae419",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict2 = vect(sent2)\n",
    "dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ea6682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the data into DataFrame\n",
    "df = pd.DataFrame([dict1, dict2])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab851a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = df.to_numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ffbc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the similarity\n",
    "cosim(ar[0], ar[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834cc168",
   "metadata": {},
   "source": [
    "__NB__: Notice the role of the fullstop at the end of the sentences!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03dcd17",
   "metadata": {},
   "source": [
    "## Method 2: Word Importance\n",
    "We define some more features of the text, which could matter for the role of the words in it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f708a73b",
   "metadata": {},
   "source": [
    "- __TF__ - term frequency -  how frequest is the appearance of a term (word) in a document\n",
    "- __DF__ - document frequency - number of documents containing the term\n",
    "- __IDF__ - inverse term frequency - how big part of all documents contain the term\n",
    "- __TF-IDF__ - an integrated measure for the iportance of a term - multiply __TF x IDF__ to find it.\n",
    "\n",
    "term = word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb9bc06",
   "metadata": {},
   "source": [
    "TF can be measured in different ways:\n",
    "- absolute number of times the word appears in a document\n",
    "- relative frequency - count of occurences divided by number of words in the document\n",
    "- logarithmically scaled frequency (e.g. log(1 + count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae47fa97",
   "metadata": {},
   "source": [
    "### 2.1 Count Vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994a6e22",
   "metadata": {},
   "source": [
    "Consider the _number of occurencies_ of a word in a document (how many times the same word appears) - raw count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c832d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create count vector from a sentence, telling the frequency of word appearance\n",
    "def cvect(sent):\n",
    "    \n",
    "    # creates the dict with the corpus as keys\n",
    "    mydict = dict.fromkeys(corpus, 0) \n",
    "    \n",
    "    # count the occurance of each word\n",
    "    for word in sent:\n",
    "        mydict[word] += 1\n",
    "    return mydict    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedad192",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = cvect(sent1)\n",
    "dict2 = cvect(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cd701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the dictionaires in a data frame\n",
    "dfc = pd.DataFrame([dict1, dict2])\n",
    "dfc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2492ae40",
   "metadata": {},
   "source": [
    "Re-calculate the similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d0c9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the binary values into array\n",
    "arc = dfc.to_numpy()\n",
    "arc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b919d6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the similarity\n",
    "cosim(arc[0], arc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4213188",
   "metadata": {},
   "source": [
    "### 2.2  Term Frequency\n",
    "Relative TF\n",
    "\n",
    "tf(t,d) = count of t in d / number of words in d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a6f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate the word appearance as a proportion of all words in the document\n",
    "def computeTF(mydict, n):\n",
    "    # New empty dict for the results of recalculation\n",
    "    tfDict = {}\n",
    "    \n",
    "    for word, wcount in mydict.items():\n",
    "        # calculate the proportion\n",
    "        tfDict[word] = wcount/float(n) \n",
    "    return(tfDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b834e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function for both sets\n",
    "tf1 = computeTF(dict1, len(sent1))\n",
    "tf2 = computeTF(dict2, len(sent2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a22340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the two vectors into dataframe\n",
    "tff = pd.DataFrame([tf1, tf2])\n",
    "tff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b0310e",
   "metadata": {},
   "source": [
    "TF doen't mean much. 10 times higher frequency doesn't mean 10 times more important term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7a429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f729569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the binary values into array\n",
    "art = tff.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d61070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the similarity\n",
    "cosim(art[0], art[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5682a49",
   "metadata": {},
   "source": [
    "### 2.3 DF and IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b72fabf",
   "metadata": {},
   "source": [
    "If a word occurs many times in one document, but also in other documents, it may not be important, but just frequent .\n",
    "IDF determines how common a word is amongst the whole corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aec5a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDF - inverse document frequency - measures the informativeness of term t\n",
    "# DF - number of documents containing the term\n",
    "# N - number of all documents\n",
    "# D - corpus of all words\n",
    "\n",
    "# idf(t, D) = N/df\n",
    "\n",
    "def computeIDF(allDocs):\n",
    "    # number of documents\n",
    "    N = len(allDocs) \n",
    "\n",
    "    # create empty dict, put the words in as keys and 0 as value\n",
    "    idf = {}\n",
    "    idf = dict.fromkeys(allDocs[0].keys(), 0)    \n",
    "    \n",
    "    # check all docs \n",
    "    for doc in allDocs:\n",
    "        # check all words \n",
    "        for word, wcount in doc.items():\n",
    "            # count the doc if the word appears in it\n",
    "            if wcount > 0:\n",
    "                idf[word] += 1\n",
    "    \n",
    "    # make correction in the counting to avoid eventual division by zero: idf(t) = log10(N/(df + 1))\n",
    "    for word, wcount in idf.items():\n",
    "        idf[word] = math.log10(N/(float(wcount)) + 1)\n",
    "        \n",
    "    return(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0161f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "idfs = computeIDF([dict1, dict2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbcf2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(idfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920d2a8c",
   "metadata": {},
   "source": [
    "### 2.4 TF-IDF\n",
    "TF-IDF determines how relevant a term is in a given document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d75bac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf-idf(t, d) = tf(t, d) * idf(t, D)\n",
    "\n",
    "def computeTFIDF(tf, idfs):\n",
    "    tfidf = {}\n",
    "    for word, wcount in tf.items():\n",
    "        tfidf[word] = wcount*idfs[word]\n",
    "    return(tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8455d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running our two sentences through the IDF:\n",
    "idf1 = computeTFIDF(tf1, idfs)\n",
    "idf2 = computeTFIDF(tf2, idfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7bfe0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# store in a dataframe\n",
    "idf= pd.DataFrame([idf1, idf2])\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8fcc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the binary values into array\n",
    "arx = idf.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de103959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the similarity\n",
    "cosim(arx[0], arx[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1567e6",
   "metadata": {},
   "source": [
    "BOW has many negatives\n",
    "- for example, it counts equaly \"John is older than Mary\" and \"Mary is older than John\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35068b01",
   "metadata": {},
   "source": [
    "## Method 3: Impoving by Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fd7e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6562a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langdetect\n",
    "from langdetect import detect, detect_langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21193e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b61a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bc97b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download da_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4a036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e022da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    # string.punctuation removes !\"#$%&\\'()*+,-./:;?@[\\\\]^_{|}~`\n",
    "    PUNCT = string.punctuation\n",
    "    text = text.translate(str.maketrans('', '', PUNCT))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30670e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    lang = detect(text)\n",
    "    if lang == 'en': \n",
    "        model = 'en_core_web_md'\n",
    "        from spacy.lang.en.stop_words import STOP_WORDS\n",
    "    elif lang == 'da': \n",
    "        model = 'da_core_news_md'\n",
    "        from spacy.lang.da.stop_words import STOP_WORDS\n",
    "    else:\n",
    "         print(\"Wrong language\")\n",
    "        \n",
    "    mysent = []\n",
    "    nlp = spacy.load(model)\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for token in doc:\n",
    "        if not (token.is_stop or token.is_punct or token.is_space):\n",
    "            mysent.append(token.text)\n",
    "    return mysent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf2d928",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2 = clean(doc2)\n",
    "tok2 = tokenize(sent2)\n",
    "tok2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a02f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recalculate the similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81cca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy.displacy.serve(doc, style=\"ent\")\n",
    "# spacy.displacy.serve(doc, style=\"dep\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
